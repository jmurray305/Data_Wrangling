{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Week 1 Assignment</center>\n",
    "In this week's FTE, we examined CSV and JSON file formats. We wrote code to manually convert a specific CSV file to a specific JSON in the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/scientists.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-83380623bf10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read CSV file. This wouldn't work well for very large files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/scientists.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/scientists.csv'"
     ]
    }
   ],
   "source": [
    "# Read CSV file. This wouldn't work well for very large files\n",
    "with open('data/scientists.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader)\n",
    "    \n",
    "# Write JSON file to disk\n",
    "with open('data/scientists.json', 'w') as f:\n",
    "    json.dump(rows, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "The comment above the CSV section makes an assumption and says it wouldn't work for large files. Use the following articles to understand the terms **eager evaluation** and **lazy evaluation**:\n",
    "* https://en.wikipedia.org/wiki/Eager_evaluation\n",
    "* https://en.wikipedia.org/wiki/Lazy_evaluation\n",
    "\n",
    "Now answer the following questions:\n",
    "\n",
    "1. In light of the two definitions above, what is the assumption made by that comment?\n",
    "2. Explain why that assumption is right or wrong.\n",
    "3. Is it safe to use the code above on large files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager Evaluation stores fully computes values before they are required. Greedy evaluation computes as soon as its arguments are known (Imperative languages are generally greedily evaluated). Greedy and Eager evaluations seem a lot alike but eager evaluation allows to create a large workload of values to be evaluated where as greed ebaluation would immediately evaluate them. \n",
    "\n",
    "**Pros**\n",
    "* More Control\n",
    "* More Transparent\n",
    "\n",
    "__Cons__\n",
    "* More responsibility for the developer\n",
    "* Potential overhead more memory intense \n",
    "\n",
    "Lazy Evaluation only computes a value when it is needed. The assumption made by that comment is that the entire file will be read directly into memeory and not used immediately. Advantages of Lazy Evaluation\n",
    "* It allows the language runtime to discard sub-expression that are not directly linked to the final result of the expression\n",
    "* It allows faster computations, only access the needed data\n",
    "* It allows the programmer to access components of data structures out-of-order after initializing them \n",
    "* Great for using not-frequently access data\n",
    "\n",
    "Eager and Lazy Evaluation really depends on the langague you are working in.\n",
    "\n",
    "\n",
    "1. In light of the two definitions above, what is the assumption made by that comment?\n",
    "* The assumption made above is that the entire file is being read and used immediately and if it is a large file it will run into memory issues. The better faster way would be to read in the file by chucks or process the data when reading in in\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "chunksize = 10 ** 8\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    "    process(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Is it safe to use the code above on large files?\n",
    "    * It really depends on the system you are working on and how large the file is. There are better/faster ways to get a file from CSV to JSON like do a json.dump while reading the file in (process the data while reading it in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Programmers hate code written for a specific case, \"I don't care if it can solve one special case, I want it to solve *all* cases.\" This generalization process is called **\"abstraction\"**.\n",
    "\n",
    "1. Generalize the CSV->JSON code above into a function that can work for any CSV file and any JSON file (within reason).\n",
    "\n",
    "Being able to convert in only one direction is only helpful half the time. Specious math aside, \n",
    "\n",
    "2. Write a generalized JSON->CSV converter function.\n",
    "\n",
    "3. Use the functions to do a \"round-trip\" (CSV->JSON->CSV or JSON->CSV->JSON) on the Consumer Complaint Database data found at https://catalog.data.gov/dataset/consumer-complaint-database#topic=consumer_navigation\n",
    "\n",
    "When you are done with the two functions, the original and the round-trip files should be reasonably identical.\n",
    "\n",
    "**Hint:** Mac and Linux have a command line tool called 'diff' that will show differences between two files. Windows users can use the 'fc' command on the command line. See this answer on StackOverflow for alternatives: https://stackoverflow.com/questions/6877238/what-is-the-windows-equivalent-of-the-diff-command\n",
    "\n",
    "**Also: No using libraries like Pandas that will automatically do the conversion!!** The purpose of this exercise is for you to get a fairly deep understanding of the two formats and using a converter will not fulfill that purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I might of gone overbroad on this section of the Assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv \n",
    "import json\n",
    "import re\n",
    "\n",
    "def jsonToCsv(jsonFile, output):\n",
    "    fileInput = jsonFile\n",
    "    fileOutput = output\n",
    "    inputFile = open(fileInput) #open json file\n",
    "    outputFile = open(fileOutput, 'w') #load csv file\n",
    "    data = json.load(inputFile) #load json content\n",
    "    inputFile.close() #close the input file\n",
    "    output = csv.writer(outputFile) #create a csv.write\n",
    "    output.writerow(data[0].keys())  # header row\n",
    "    for row in data:\n",
    "        output.writerow(row.values()) #values row\n",
    "    outputFile.close()\n",
    "        \n",
    "def csvToJson(csvFile,output):\n",
    "    fileInput = csvFile\n",
    "    fileOutput = output\n",
    "    inputFile = open(fileInput) #open csv file\n",
    "    outputFile = open(fileOutput, 'w') #open json/output file\n",
    "    lines= []\n",
    "    for line in inputFile:\n",
    "        line = line.replace(\"\\n\",\"\") #remove newline chars from the end of each rec\n",
    "        lines.append(line) # read each line into lines list\n",
    "    header = lines[0].split(\",\")\n",
    "    for i in lines[1:]:\n",
    "        data_line = i.split(\",\")\n",
    "        data_dict = dict(zip(header,data_line))\n",
    "        json.dump(data_dict, outputFile) \n",
    "    outputFile.close()\n",
    "        \n",
    "def jsonRegex(infile, output):\n",
    "    '''\n",
    "    jsonRegex(infile,output) expects a file from CSVTOJSON function.\n",
    "    jsonRegex takes in a the formated file from csv2json and adds a [ at the beginning of the file\n",
    "    and formats between the }{ to add a comma },{. It also adds the closing bracket ] at the end of \n",
    "    the file. Current is is added to a new line. currently working on getting it to the end of the last\n",
    "    entry\n",
    "    '''\n",
    "    fileInput = infile\n",
    "    fileOutput = output\n",
    "    inputfile = open(fileInput,'r')\n",
    "    outputfile= open(fileOutput, 'w')\n",
    "    outputfile.write(\"[\")\n",
    "    for x in inputfile:\n",
    "        outputfile.write(x.replace(\"}{\",\"},{\"))\n",
    "    outputfile.write(\"]\")\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonToCsv(\"scientists.json\",\"json2csv.tst\")\n",
    "csvToJson(\"json2csv.tst\",\"csv2json.tst\")\n",
    "jsonRegex(\"csv2json.tst\", \"csv2json2.tst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
