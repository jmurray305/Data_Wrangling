{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Working with Excel files\n",
    "\n",
    "\n",
    "Overview:\n",
    "* Reading and writing Excel files with Pandas\n",
    "* Reading and writing Excel files with openpyxl\n",
    "* Aside: Accessing the SQLite3 database with Python\n",
    "* Aside 2: The *Dataset* library\n",
    "* Use Case 1: Translate spreadsheet into database\n",
    "* Use Case 2: Update spreadsheet from database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing Excel files with Pandas\n",
    "\n",
    "In what could be considered by some to be a cruel trick, we will only mention here that Pandas has a read_excel() function that will give you a dataframe made from the Excel data. As with all things, there are pros and cons to accessing Excel data this way but it is out of the scope of this FTE to spend time enumerating them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing Excel files with openpyxl\n",
    "\n",
    "Openpyxl is a Python library devoted to reading and manipulating Excel files. I believe Anaconda Python has it installed but if you don't have it, you can install it with conda or pip. \n",
    "\n",
    "Just as Excel's main unit of work is the workbook, openpyxl has a `load_workbook()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = load_workbook(filename=\"data/aapl_HistoricalQuotes_10yr.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the workbook's sheet names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aapl_HistoricalQuotes_10yr']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workbook.sheetnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And decide to use the active one. \n",
    "\n",
    "It is worth noting that it is, of course, possible to use other sheets but we only have one in this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aapl_HistoricalQuotes_10yr'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet = workbook.active\n",
    "\n",
    "sheet.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reference individual cells in the sheet by using dictionary notation, with the row and column put together in the square brackets. \n",
    "\n",
    "That sounded much more confusing than it is. To look at row 1's A column, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Cell 'aapl_HistoricalQuotes_10yr'.A1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet['A1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that wasn't very useful. Let's try looking at the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet['A1'].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better!\n",
    "\n",
    "We can also ask for cells by row and column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205.72"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet.cell(row=6, column=5).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also work with ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<Cell 'aapl_HistoricalQuotes_10yr'.A1>,\n",
       "  <Cell 'aapl_HistoricalQuotes_10yr'.B1>,\n",
       "  <Cell 'aapl_HistoricalQuotes_10yr'.C1>),\n",
       " (<Cell 'aapl_HistoricalQuotes_10yr'.A2>,\n",
       "  <Cell 'aapl_HistoricalQuotes_10yr'.B2>,\n",
       "  <Cell 'aapl_HistoricalQuotes_10yr'.C2>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet[\"A1:C2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating\n",
    "\n",
    "If you took MSDE 620, you may remember we discussed \"iterators,\" and \"iterating,\" and other forms of the word. The tl;dr version is that iterating means using a loop. \n",
    "\n",
    "As we saw up above, it is possible to grab cell objects but for our purposes, getting the values is more important. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('date', 'close', 'volume', 'open', 'high', 'low')\n",
      "(datetime.datetime(2019, 9, 4, 0, 0), 209.19, 19216820, 208.39, 209.48, 207.32)\n",
      "(datetime.datetime(2019, 9, 3, 0, 0), 205.7, 20059570, 206.43, 206.98, 204.22)\n",
      "(datetime.datetime(2019, 8, 30, 0, 0), 208.74, 21162560, 210.16, 210.45, 207.2)\n",
      "(datetime.datetime(2019, 8, 29, 0, 0), 209.01, 21007650, 208.5, 209.32, 206.655)\n"
     ]
    }
   ],
   "source": [
    "for value in sheet.iter_rows(min_row=1,\n",
    "                             max_row=5,\n",
    "                             values_only=True):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is nothing magical about the variable name `value` above. It is just descriptive of what it will hold.\n",
    "\n",
    "The `value` variable is special, though, in that it is a **tuple**. Remember, tuples are just like lists except you can't change the values inside. \n",
    "\n",
    "So, if `value` is just like a list, we should be able to index it like a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date  close   volume   open   high   low   \n",
      "2019-09-04 00:00:00  209.19   19216820   208.39   209.48   207.32   \n",
      "2019-09-03 00:00:00  205.7   20059570   206.43   206.98   204.22   \n",
      "2019-08-30 00:00:00  208.74   21162560   210.16   210.45   207.2   \n",
      "2019-08-29 00:00:00  209.01   21007650   208.5   209.32   206.655   \n"
     ]
    }
   ],
   "source": [
    "for value in sheet.iter_rows(min_row=1,\n",
    "                             max_row=5,\n",
    "                             values_only=True):\n",
    "    print(f'{value[0]}  {value[1]}   {value[2]}   {value[3]}   {value[4]}   {value[5]}   ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned how to get to the data in the spreadsheet, let's learn the basics of SQL and relational databases. \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"padding-right:10px;\" src=\"figures/sqlite370_banner.gif\" width=200><br>\n",
    "## Aside: Accessing the SQLite3 database with Python\n",
    "<div style=\"text-align: right\">\n",
    "https://www.sqlite.org/index.html\n",
    "</div>\n",
    "<br>\n",
    "SQLite is a file-based database, meaning there is no \"Database Management\" program, like Oracle or MySQL. There is only the data file on disk that needs to be loaded. This is the beauty of SQLite -- you can include it as a storage solution in a program without having to install an RDBMS. \n",
    "\n",
    "When accessing a database programmatically, regardless of language, the same basic set of steps need to be performed, at least to start:\n",
    "\n",
    "1. Connect to database - this will give a \"connection object\" or **handle** to access the DB internals.\n",
    "2. Use the connection object's functions to run queries -- **save results to a variable**.\n",
    "3. The result variable will be organized as rows and columns and can be iterated through.\n",
    "\n",
    "First, we will create two DataFrames from dictionaries that represent our employee and department tables. Then we will create a new `emp.db` database file and tell the DataFrames to convert (write) them to SQLite tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**employees**\n",
    "\n",
    "emp_id | emp_name | dept_id\n",
    "--|-----|-----------\n",
    "1 | Tom | 1\n",
    "2 | Mary | 2\n",
    "3 | John | 3\n",
    "4 | Tim | 1\n",
    "5 | Jenny | \n",
    "\n",
    "NOTE: Jenny is a new hire not assigned a department yet.<br>\n",
    "**departments**\n",
    "\n",
    "dept_id | dept_name\n",
    "--------|----------\n",
    "1 | HR\n",
    "2 | Development\n",
    "3 | Marketing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   emp_id emp_name  dept_id\n",
      "0       1      Tom        1\n",
      "1       2     Mary        2\n",
      "2       3     John        3\n",
      "3       4      Tim        1\n",
      "4       5    Jenny        0\n",
      "\n",
      "   dept_id    dept_name\n",
      "0        1           HR\n",
      "1        2  Development\n",
      "2        3    Marketing\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Have to put a 0 in for Jenny, then adjust it.\n",
    "emp_df = pd.DataFrame({'emp_id':[1,2,3,4,5], 'emp_name':['Tom', 'Mary','John', 'Tim', 'Jenny'], 'dept_id':[1,2, 3, 1, 0] })\n",
    "dept_df = pd.DataFrame({'dept_id':[1,2,3], 'dept_name':['HR','Development', 'Marketing']})\n",
    "\n",
    "# Workaround to get a null into Jenny's dept_id\n",
    "# emp_df.loc[emp_df['emp_name'] == 'Jenny', 'dept_id'] = np.nan\n",
    "\n",
    "print(f'{emp_df}\\n\\n{dept_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick aside:** If we try to create tables in a database where they already exist, we will get a ValueError, like we see below (if you are running this code and the cell below doesn't give an error, run it twice).\n",
    "\n",
    "This can be handled in two ways:\n",
    "\n",
    "* Use __exception handling__ e.g. a try/except block (below)\n",
    "* Drop the table(s) and recreate (example in Use Cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Table 'employees' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-31a37ab7a79f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/emp.db\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memp_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'employees'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdept_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'departments'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2529\u001b[0m         sql.to_sql(self, name, con, schema=schema, if_exists=if_exists,\n\u001b[1;32m   2530\u001b[0m                    \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                    dtype=dtype, method=method)\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m     def to_pickle(self, path, compression='infer',\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m    458\u001b[0m     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n\u001b[1;32m    459\u001b[0m                       \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                       chunksize=chunksize, dtype=dtype, method=method)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   1544\u001b[0m                             \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m                             dtype=dtype)\n\u001b[0;32m-> 1546\u001b[0;31m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1547\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fail'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 575\u001b[0;31m                     \"Table '{name}' already exists.\".format(name=self.name))\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Table 'employees' already exists."
     ]
    }
   ],
   "source": [
    "# Making connection to database\n",
    "# con will be our connection \"handle\" \n",
    "con = sqlite3.connect(\"data/emp.db\")\n",
    "\n",
    "emp_df.to_sql('employees', con)\n",
    "dept_df.to_sql('departments', con)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A try/except block works exactly as it sounds -- **try** to run some risky code. Catch and handle any **except**ion that happens. \n",
    "\n",
    "Virtually every modern language has some form of exception handling to catch errors. \n",
    "\n",
    "**NOTE:** Exceptions are different from code syntax errors. An exception happens when the code is technically correct but something unexpected happens -- wrong type of value in a variable, file exists or doesn't exist (depending on operation), etc. \n",
    "\n",
    "Here is how we could handle the database error above:\n",
    "\n",
    "```\n",
    "try:\n",
    "    emp_df.to_sql('employees', con)\n",
    "    dept_df.to_sql('departments', con)\n",
    "except ValueError as v_error:\n",
    "    print(v_error)\n",
    "```\n",
    "\n",
    "Try to create the tables, but if we get a ValueError, print the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'employees' already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    emp_df.to_sql('employees', con)\n",
    "    dept_df.to_sql('departments', con)\n",
    "except ValueError as v_error:\n",
    "    print(v_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing DB Data from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDL and DML\n",
    "\n",
    "SQL is actually composed of two languages:\n",
    "* DDL - Data Definition Language, used to create tables, etc.\n",
    "* DML - Data Manipulation Language, used to insert, update and retrieve data from the tables.\n",
    "\n",
    "We were able to skip the table creation phase above using Pandas to write our dataframes. \n",
    "\n",
    "When working with databases, by far the most common operation is retrieving (SELECTing) data, followed by INSERTing and UPDATEing. Let's look at SELECT first.\n",
    "\n",
    "### SQL SELECT\n",
    "\n",
    "When SQL first came out, lower case letters hadn't been invented yet (joke!), so by convention, SQL keywords like SELECT, FROM, WHERE, etc. are written in all caps. However, to an old Unix/C programmer like me, caps make my skin crawl. In reality, the database won't care about caps or no caps so I may forget to use them.\n",
    "\n",
    "SELECT statements are composed of 3 parts:\n",
    "\n",
    "1. The keyword SELECT followed by a list of columns to display\n",
    "2. The keyword FROM followed by the table name(s) from which to get the data\n",
    "3. The keyword WHERE followed by logical statement that acts as a **filter** for the data. We'll look at this in depth in a moment.\n",
    "\n",
    "Written in a more \"normal\" form, it looks like this:\n",
    "\n",
    "```\n",
    "SELECT column a, column b, column c \n",
    "FROM table\n",
    "WHERE column = value;\n",
    "```\n",
    "\n",
    "To use our example \"employees\" table from above: \n",
    "`SELECT emp_id, emp_name FROM employees where dept_id = 1;`\n",
    "\n",
    "This would return to us a table of results showing \n",
    "\n",
    "|emp_id|emp_name|\n",
    "|------|--------|\n",
    "| 1 | Tom |\n",
    "| 4 | Tim |\n",
    "\n",
    "That gives us enough information to ask the database to tell us all its table names.\n",
    "\n",
    "### Querying the master table \n",
    "\n",
    "If this was the first time seeing the database, you might not know much about it and need to get some info.\n",
    "\n",
    "All DB systems have some sort of \"Master\" table(s) that keeps track of the database **schema** -- structural info about the DB architecture. \n",
    "\n",
    "A really useful query is to ask the \"Master\" for all the (other) table names. We will do this using the connection object's `execute()` function to pass raw SQL to the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = con.execute('select name from sqlite_master where type = \"table\";')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I'm asking the `sqlite_master` table to return the `name` column contents for every row whose `type` column has the word \"table\" in it. \n",
    "\n",
    "Go back and read that sentence again and make sure you understand how my verbal description matches the SQL. It is basically a long-winded way of saying 'return all table names from the sqlite_master table.'\n",
    "\n",
    "Results are returned to your Python code as `cursor` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlite3.Cursor"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above shows that the `cursor` variable containing the query result is a **Cursor** data type. Database cursors are the table-like returns from a query. To see the query results, we can just ask the cursor to `fetchall()` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('employees',), ('departments',)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that we have 2 tables in our database, **\"employees\"** and **\"departments\"**. \n",
    "\n",
    "You could also get a list of column names from the master table but then you wouldn't necessarily know what table they live in. Instead, let's query a table and use the result cursor's `description` property. The first entry in the description is the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = con.execute('select * from employees;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('index', None, None, None, None, None, None),\n",
       " ('emp_id', None, None, None, None, None, None),\n",
       " ('emp_name', None, None, None, None, None, None),\n",
       " ('dept_id', None, None, None, None, None, None))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we just want a list of column names for the employee table, we could use one of three different approaches: \n",
    "* a for loop\n",
    "* a list comprehension \n",
    "* map with lambda function \n",
    "\n",
    "We will look at all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "emp_id\n",
      "emp_name\n",
      "dept_id\n"
     ]
    }
   ],
   "source": [
    "# Regular for loop\n",
    "for col in cursor.description:\n",
    "    print(col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension \n",
    "emp_cols = [col[0] for col in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index', 'emp_id', 'emp_name', 'dept_id']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index', 'emp_id', 'emp_name', 'dept_id']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map function - Python 3 map is a special type\n",
    "# so we wrap it in a list to be able to \"look inside\" it.\n",
    "emp_cols2 = list(map(lambda x: x[0], cursor.description))\n",
    "\n",
    "emp_cols2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to list tables and columns, we can select data from individual tables or do a join to replace some of those xxx_id columns with actual words.\n",
    "\n",
    "First, let's do some basic `select` statements.\n",
    "\n",
    "In SQL, the '\\*' (asterisk - is a wildcard character -- it means 'Give me everything' and the most basic query you can do is:\n",
    "\n",
    "`SELECT * FROM table;`\n",
    "\n",
    "Notice there is no 'WHERE' clause. That means the query will return **all** rows in the table. \n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 'Tom', 1)\n",
      "(1, 2, 'Mary', 2)\n",
      "(2, 3, 'John', 3)\n",
      "(3, 4, 'Tim', 1)\n",
      "(4, 5, 'Jenny', 0)\n"
     ]
    }
   ],
   "source": [
    "data = con.execute('select * from employees;')\n",
    "for line in data:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how that gave us all rows and all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = con.execute('select employees.emp_name, d.dept_name from employees join departments d on employees.dept_id = d.dept_id;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom\t\tHR\n",
      "Mary\t\tDevelopment\n",
      "John\t\tMarketing\n",
      "Tim\t\tHR\n"
     ]
    }
   ],
   "source": [
    "for line in data:\n",
    "    print(f'{line[0]}\\t\\t{line[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use tricks like above to get the column names.\n",
    "\n",
    "Let's do a search for all employees whose name starts with 'T'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tom',), ('Tim',)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('select emp_name from employees where emp_name like \"T%\";').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting data\n",
    "\n",
    "You can put data into a database with the `INSERT` statement. The syntax is a bit different from a `SELECT`. \n",
    "\n",
    "```\n",
    "INSERT into table_name (column1, column2, column3, etc.) VALUES (value1, value2, value3, etc.);\n",
    "```\n",
    "The column list does not have to be the same order as the actual table, but, the values must be in the same order as the columns in the statement. Let's INSERT a new employee:\n",
    "\n",
    "| Index | emp_id | emp_name | dept_id |\n",
    "|-------|--------|----------|---------|\n",
    "| 5     | 6      | Jeff Davis | 1     |\n",
    "\n",
    "**NOTE:** Pandas inserted the index number automatically but normally we wouldn't use it. Similarly, emp_id would be auto-generated by the database at time of insertion, not assigned by the user. \n",
    "\n",
    "The INSERT statement for that employee would look like this:\n",
    "\n",
    "```\n",
    "INSERT into employees ('index', 'emp_id', 'emp_name', 'dept_id') VALUES (5, 6, 'Jeff Davis', 1);\n",
    "```\n",
    "Now, let's actually do that in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"INSERT into employees ('index', 'emp_id', 'emp_name', 'dept_id') VALUES (5, 6, 'Jeff Davis', 1);\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a CURSOR is returned but it is empty. Let's look at the table and verify our employee was inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 'Tom', 1),\n",
       " (1, 2, 'Mary', 2),\n",
       " (2, 3, 'John', 3),\n",
       " (3, 4, 'Tim', 1),\n",
       " (4, 5, 'Jenny', 0),\n",
       " (5, 6, 'Jeff Davis', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('select * from employees;').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, if you try to insert the same data more than once, a well-designed database will give an error. In that case, you should `DELETE` the row before trying to `INSERT` again. Syntax for the `DELETE` statement is as follows:\n",
    "\n",
    "```\n",
    "DELETE from table_name WHERE condition;\n",
    "```\n",
    "**IMPORTANT NOTE:** Notice the `DELETE` command has a `WHERE` clause. This is to filter/select only the proper rows for deletion. **If you forget the `WHERE` clause, it will delete all the rows in the table!** Trust me. I've done it. Not fun to explain to your boss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 'Tom', 1),\n",
       " (1, 2, 'Mary', 2),\n",
       " (2, 3, 'John', 3),\n",
       " (3, 4, 'Tim', 1),\n",
       " (4, 5, 'Jenny', 0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute('delete from employees where emp_id = 6;')\n",
    "con.execute('select * from employees;').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put Jeff Davis back in, then use an `UPDATE` to change his name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 'Tom', 1),\n",
       " (1, 2, 'Mary', 2),\n",
       " (2, 3, 'John', 3),\n",
       " (3, 4, 'Tim', 1),\n",
       " (4, 5, 'Jenny', 0),\n",
       " (5, 6, 'Jeff Davis', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"INSERT into employees ('index', 'emp_id', 'emp_name', 'dept_id') VALUES (5, 6, 'Jeff Davis', 1);\")\n",
    "con.execute('select * from employees;').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax for the `UPDATE` is a bit of a mix of `SELECT` and `INSERT`.\n",
    "\n",
    "```\n",
    "UPDATE table_name set column1 = value, column2 = value, etc., WHERE condition;\n",
    "```\n",
    "\n",
    "Since you have a `WHERE` clause, you can pick and choose many rows to update, for example if we were updating a state_abbreviation table:\n",
    "\n",
    "```\n",
    "UPDATE state_abbreviation set abbrev = 'CO' where abbrev = 'Colo';\n",
    "```\n",
    "Notice it is perfectly OK to update the same column that is in the `WHERE` clause. \n",
    "\n",
    "If you only want to update one row, you need something unique about that row. Usually, that is where the table's \\_id column comes into play. Let's use emp_id to change Jeff Davis to a different department. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 'Tom', 1),\n",
       " (1, 2, 'Mary', 2),\n",
       " (2, 3, 'John', 3),\n",
       " (3, 4, 'Tim', 1),\n",
       " (4, 5, 'Jenny', 0),\n",
       " (5, 6, 'Jeff Davis', 2)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\"UPDATE employees set dept_id = 2 where emp_id = 6;\")\n",
    "con.execute('select * from employees;').fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is good form to close database connections when done. If you forget, you might be wasting system resources (memory leak, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just knowing that little bit will account for 90% of everything you'll generally have to do with databases.\n",
    "\n",
    "In a moment, we will look at a couple of use cases where we can put it to work.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside 2: The Dataset Library\n",
    "\n",
    "The optional book, *'Data Wrangling with Python'* mentions the Dataset library (https://dataset.readthedocs.io/en/latest/)to mitigate much of the SQL complexity we see above. Dataset says it is \"databases for lazy people.\"\n",
    "\n",
    "Let's take a quick look at Dataset so we can use it in the Use Cases.\n",
    "\n",
    "Dataset can be installed as normal, using conda or pip.\n",
    "\n",
    "We will import the library and then work with the employees database from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "\n",
    "db = dataset.connect(\"sqlite:///data/emp.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a listing of tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['departments', 'employees']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a listing of columns in the employees table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['index', 'emp_id', 'emp_name', 'dept_id']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['employees'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the department names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "Development\n",
      "Marketing\n"
     ]
    }
   ],
   "source": [
    "for dept in db['departments']:\n",
    "    print(dept['dept_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what is the department ID for Development?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('index', 1), ('dept_id', 2), ('dept_name', 'Development')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['departments'].find_one(dept_name='Development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert a new row into a table by passing a dictionary into the `insert()` method. Let's add an employee. First, I want to look at what is in that table. This is feasible since our table is so small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('index', 0), ('emp_id', 1), ('emp_name', 'Tom'), ('dept_id', 1)])\n",
      "OrderedDict([('index', 1), ('emp_id', 2), ('emp_name', 'Mary'), ('dept_id', 2)])\n",
      "OrderedDict([('index', 2), ('emp_id', 3), ('emp_name', 'John'), ('dept_id', 3)])\n",
      "OrderedDict([('index', 3), ('emp_id', 4), ('emp_name', 'Tim'), ('dept_id', 1)])\n",
      "OrderedDict([('index', 4), ('emp_id', 5), ('emp_name', 'Jenny'), ('dept_id', 0)])\n"
     ]
    }
   ],
   "source": [
    "for emp in db['employees'].all():\n",
    "    print(emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's insert Jeff again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db['employees'].insert(dict(index=5, emp_id=6, emp_name='Jeff', dept_id=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to double-check it went right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('index', 0), ('emp_id', 1), ('emp_name', 'Tom'), ('dept_id', 1)])\n",
      "OrderedDict([('index', 1), ('emp_id', 2), ('emp_name', 'Mary'), ('dept_id', 2)])\n",
      "OrderedDict([('index', 2), ('emp_id', 3), ('emp_name', 'John'), ('dept_id', 3)])\n",
      "OrderedDict([('index', 3), ('emp_id', 4), ('emp_name', 'Tim'), ('dept_id', 1)])\n",
      "OrderedDict([('index', 4), ('emp_id', 5), ('emp_name', 'Jenny'), ('dept_id', 0)])\n",
      "OrderedDict([('index', 5), ('emp_id', 6), ('emp_name', 'Jeff'), ('dept_id', 2)])\n"
     ]
    }
   ],
   "source": [
    "for emp in db['employees'].all():\n",
    "    print(emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be the majority of what we need for the Use Cases.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases\n",
    "\n",
    "Note: The first time we run database code from Use Case 1, the SQLite file will be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1: Translate spreadsheet into database\n",
    "\n",
    "Let's assume for a moment that your manager receives a spreadsheet of data every week that you want to save for further analysis. A little Python code that reads directly out of the Excel file and inserts in the database is a great way to go. \n",
    "\n",
    "We will use openpyxl and dataset, combining all that we did above.\n",
    "\n",
    "We have several Excel files in the data directory, each one representing one year's worth of Apple stock data. We will **create** the database here with the 2009 data. It will be your assignment to get all the rest of the data into the database.\n",
    "\n",
    "Even though there are only 10 files, we want to get in the habit of programmatically building the file names. Think of it this way: today we *only* have 10 files. What if your data set had 100 files? Or 1000? I'll give you some hints as we work through it. \n",
    "\n",
    "You may notice that all the file names follow the same pattern: YYYY_aapl_data.xlsx where YYYY is a year value. This was done intentionally and you will often find data files will have some sort of logical naming scheme. Think of it like this: you want to be able to programmatically create the names to read them, and some developer wanted to programmatically create the names to write them. Many times data files will be the result of some automated process that has very little human intervention. \n",
    "\n",
    "Looking again at those names, you may notice that the only thing that changes is the year. That means we can create a variable to hold the constant part. Also, we might as well put the database file name into a variable, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = '_aapl_data.xlsx'\n",
    "db_file = 'stock_prices.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use Dataset to create the connection. Dataset will create the .db file if it doesn't exist but we won't have the normal database safeguards in place to guard against duplicates, so we'll have to be careful about re-running the next few cells multiple times.\n",
    "\n",
    "Notice how we join a string to the file name variable to get the complete connection info for Dataset. \n",
    "\n",
    "**HINT:** This technique might be useful in your homework!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dataset.connect(\"sqlite:///\" + db_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our database should be completely empty. We should confirm that fact and if it **is** empty, we can create the table. If it isn't we should drop and recreate the table. This isn't something we would normally do in a production environment but while we are learning we could be rerunning the code many times and databases generally hate inserting duplicate data.\n",
    "\n",
    "To accomplish this, I'm going to do just about the simplest check possible. I'm going to ask for the table names and if the list comes back empty, then the database must be empty. If the list has something in it, I'll drop the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a for loop just in case someone snuck in a new table on us\n",
    "if (len(db.tables) > 0):\n",
    "    for table in db.tables:\n",
    "        db[table].drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty table. We'll create the columns after that from the spreadsheet's first row.\n",
    "# The create_table function returns the table so we automatically have a handle\n",
    "table = db.create_table(\"aapl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's open our 2009 data, get the header and create table columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2009'\n",
    "data_file = \"data/\" + year + input_name\n",
    "workbook = load_workbook(filename=data_file)\n",
    "sheet = workbook.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'date'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = sheet[1]\n",
    "header[0].value     # This is just to sanity-check that we got the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, everything is just about ready. We have to give the columns a type when we create them but that isn't a problem since we **inspected the data in a text editor.** All of the columns are floating point except for 'date' which is, predictably, a datetime type. \n",
    "\n",
    "For this handful of columns we could have individual create statements, but that's the amateur way to do it! Probably the best way to figure out many column types would be to read the second row and loop through the cells, detecting the type and storing it in a list. The problem is that everything read in from a file will be a string, so we need to detect what type is hiding in the string. Fortunately, the string type has some allies for us. \n",
    "\n",
    "`isdigit()` will return `True` if the string is an integer, but will give `False` if there is a decimal point. The `float()` function will throw a ValueError if there are letters in the string, and our dates always have '/' in them. \n",
    "\n",
    "That should be enough to figure out what we have. Maybe we should test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "# Note: These have to be tested in the right order. isfloat() reports True for integers.\n",
    "def get_type(value):\n",
    "    if value.isdigit():\n",
    "        return dataset.types.Integer\n",
    "    elif isfloat(value):\n",
    "        return dataset.types.Float\n",
    "    elif '/' in value:\n",
    "        return dataset.types.Date\n",
    "    else:\n",
    "        return dataset.types.Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sqlalchemy.sql.sqltypes.Unicode'>\n",
      "<class 'sqlalchemy.sql.sqltypes.Unicode'>\n",
      "<class 'sqlalchemy.sql.sqltypes.Float'>\n",
      "<class 'sqlalchemy.sql.sqltypes.Integer'>\n",
      "<class 'sqlalchemy.sql.sqltypes.Date'>\n"
     ]
    }
   ],
   "source": [
    "print(get_type('abc'))\n",
    "print(get_type('ab.c'))\n",
    "print(get_type('1.1'))\n",
    "print(get_type('1'))\n",
    "print(get_type('1/1/2019'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks pretty good. Let's try it out on a real row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009/12/31 is <class 'sqlalchemy.sql.sqltypes.Date'>\n",
      "30.1046 is <class 'sqlalchemy.sql.sqltypes.Float'>\n",
      "87907426.0000 is <class 'sqlalchemy.sql.sqltypes.Float'>\n",
      "30.4471 is <class 'sqlalchemy.sql.sqltypes.Float'>\n",
      "30.4786 is <class 'sqlalchemy.sql.sqltypes.Float'>\n",
      "30.0800 is <class 'sqlalchemy.sql.sqltypes.Float'>\n"
     ]
    }
   ],
   "source": [
    "row2 = sheet[2]\n",
    "for cell in row2:\n",
    "    print(f'{cell.value} is {get_type(cell.value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work! You might want to keep `get_type()` and `isfloat()` where you can find them. They could come in handy someday.\n",
    "\n",
    "We are finally ready to create the columns of the database! The procedure is similar to the cell above, except well will use Dataset's `create_column()` function instead of `print()` in the loop. The syntax is:\n",
    "\n",
    "`table.create_column(name, type)`\n",
    "\n",
    "Where **name** is the name of the column in the database, and **type** corresponds to a Dataset definition (that also corresponds to types defined in the *very* popular library, SQLAlchemy). This is why we had `get_type()` return the Dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: header contains the first row cells that are column names\n",
    "# the enumerate function outputs a number to the variable index\n",
    "# index \n",
    "for index, col_name in enumerate(header):\n",
    "    table.create_column(col_name.value, get_type(row2[index].value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then check your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'date', 'close', 'volume', 'open', 'high', 'low']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID is a unique number generated by the database. We can ignore it when doing inserts.\n",
    "\n",
    "Speaking of inserts, it is just about that time. Dataset inserts a dictionary as a row so our task will be to loop through each row of the worksheet and construct a dictionary from the cells. The dictionary keys are the column names (which is conveniently stored in the `table.columns` **list**) and the values are the cell values.\n",
    "\n",
    "Each worksheet has a `values` property we can use for our loop's iterable to only return values. This property is just a big block of rows so if we want to slice off the header we have to convert it to something list-like:\n",
    "\n",
    "`tuple(sheet.values)[1:]`\n",
    "\n",
    "We can build our dictionary from that. Remember we don't have to insert the `id` column so we will slice it and remove the `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,date\n",
    "keys = table.columns[1:]\n",
    "\n",
    "for values in list(sheet.values)[1:]:\n",
    "    row = []\n",
    "    row.append(datetime.strptime(values[0],'%Y/%m/%d').date())\n",
    "    row = row + list(values[1:])\n",
    "    d_row = dict(zip(keys,row))\n",
    "    table.insert(d_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, we can see if the number of worksheet rows matches the number of table rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in worksheet (plus header): 83\n",
      "Rows in database: 82\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows in worksheet (plus header): {len(sheet[\"A\"])}' )\n",
    "print(f'Rows in database: {len(table)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! You'll get a chance to practice with the 2010 to 2019 data files in the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Update spreadsheet from database\n",
    "\n",
    "Now, let's assume the opposite case: you have stock data streaming into a database and your boss wants a spreadsheet every week with year-to-date data.\n",
    "\n",
    "First, a quick, little function to print all the rows of a worksheet, called `print_rows()`, then we will create a new worksheet and query our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rows():\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at creating a new worksheet and adding a header and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.chart import BarChart, Reference\n",
    "\n",
    "workbook = Workbook() # Create a new workbook\n",
    "sheet = workbook.active # Get the active worksheet\n",
    "\n",
    "# Let's create some sample stock data using nested lists\n",
    "# rows is a list, and each element of rows is a list of cells.\n",
    "rows = [\n",
    "    ['date', 'close', 'volume', 'open', 'high', 'low'],\n",
    "    ['2019/09/04',209.1900,19216820.0000,208.3900,209.4800,207.3200],\n",
    "    ['2019/09/03',205.7000,20059570.0000,206.4300,206.9800,204.2200],\n",
    "    ['2019/08/30',208.7400,21162560.0000,210.1600,210.4500,207.2000],\n",
    "    ['2019/08/29',209.0100,21007650.0000,208.5000,209.3200,206.6550],\n",
    "    ['2019/08/28',205.5300,15957630.0000,204.1000,205.7200,203.3200],\n",
    "    ['2019/08/27',204.1600,25897340.0000,207.8600,208.5500,203.5300],\n",
    "    ['2019/08/26',206.4900,26066130.0000,205.8600,207.1900,205.0573],\n",
    "]\n",
    "\n",
    "for row in rows:\n",
    "    sheet.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('date', 'close', 'volume', 'open', 'high', 'low')\n",
      "('2019/09/04', 209.19, 19216820.0, 208.39, 209.48, 207.32)\n",
      "('2019/09/03', 205.7, 20059570.0, 206.43, 206.98, 204.22)\n",
      "('2019/08/30', 208.74, 21162560.0, 210.16, 210.45, 207.2)\n",
      "('2019/08/29', 209.01, 21007650.0, 208.5, 209.32, 206.655)\n",
      "('2019/08/28', 205.53, 15957630.0, 204.1, 205.72, 203.32)\n",
      "('2019/08/27', 204.16, 25897340.0, 207.86, 208.55, 203.53)\n",
      "('2019/08/26', 206.49, 26066130.0, 205.86, 207.19, 205.0573)\n"
     ]
    }
   ],
   "source": [
    "print_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be fairly obvious at this point how to proceed:\n",
    "\n",
    "1. Open the database connection (with or without Dataset)\n",
    "2. Query the database to get a collection of database\n",
    "3. Convert the db data into a list of lists\n",
    "4. Create a new workbook and worksheet\n",
    "5. Get the table column names as a header\n",
    "6. Append the header and rows to the worksheet\n",
    "7. Save the worksheet as an Excel file.\n",
    "\n",
    "Let's go.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aapl']\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "db = dataset.connect(\"sqlite:///stock_prices.db\")\n",
    "print(db.tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we have our datafile open and found the name of the table. Let's do a couple of queries to see how data comes back to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('id', 1), ('date', datetime.date(2009, 12, 31)), ('close', 30.1046), ('volume', 87907426.0), ('open', 30.4471), ('high', 30.4786), ('low', 30.08)])\n",
      "OrderedDict([('id', 2), ('date', datetime.date(2009, 12, 30)), ('close', 30.2343), ('volume', 102705781.0), ('open', 29.8328), ('high', 30.2857), ('low', 29.7586)])\n",
      "OrderedDict([('id', 3), ('date', datetime.date(2009, 12, 29)), ('close', 29.8714), ('volume', 110755363.0), ('open', 30.3757), ('high', 30.3886), ('low', 29.8186)])\n",
      "OrderedDict([('id', 4), ('date', datetime.date(2009, 12, 28)), ('close', 30.23), ('volume', 160784168.0), ('open', 30.2457), ('high', 30.5643), ('low', 29.9444)])\n",
      "OrderedDict([('id', 5), ('date', datetime.date(2009, 12, 24)), ('close', 29.8628), ('volume', 125222058.0), ('open', 29.0786), ('high', 29.9071), ('low', 29.05)])\n",
      "OrderedDict([('id', 6), ('date', datetime.date(2009, 12, 23)), ('close', 28.8714), ('volume', 86118086.0), ('open', 28.75), ('high', 28.9114), ('low', 28.6871)])\n",
      "OrderedDict([('id', 7), ('date', datetime.date(2009, 12, 22)), ('close', 28.6228), ('volume', 87148416.0), ('open', 28.4914), ('high', 28.6928), ('low', 28.38)])\n",
      "OrderedDict([('id', 8), ('date', datetime.date(2009, 12, 21)), ('close', 28.3186), ('volume', 152166116.0), ('open', 28.0071), ('high', 28.5357), ('low', 27.9528)])\n",
      "OrderedDict([('id', 9), ('date', datetime.date(2009, 12, 18)), ('close', 27.9186), ('volume', 151863506.0), ('open', 27.5957), ('high', 27.929), ('low', 27.5143)])\n",
      "OrderedDict([('id', 10), ('date', datetime.date(2009, 12, 17)), ('close', 27.4086), ('volume', 96720359.0), ('open', 27.7514), ('high', 27.8571), ('low', 27.2857)])\n"
     ]
    }
   ],
   "source": [
    "# Just show everything\n",
    "for i, row in enumerate(db['aapl']):\n",
    "    if i < 10:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, it comes back as a type of dictionary. That means we can use dictionary syntax to get the data out of the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-12-31 30.1046 87907426.0 30.4471 30.4786 30.08\n",
      "2009-12-30 30.2343 102705781.0 29.8328 30.2857 29.7586\n",
      "2009-12-29 29.8714 110755363.0 30.3757 30.3886 29.8186\n",
      "2009-12-28 30.23 160784168.0 30.2457 30.5643 29.9444\n",
      "2009-12-24 29.8628 125222058.0 29.0786 29.9071 29.05\n",
      "2009-12-23 28.8714 86118086.0 28.75 28.9114 28.6871\n",
      "2009-12-22 28.6228 87148416.0 28.4914 28.6928 28.38\n",
      "2009-12-21 28.3186 152166116.0 28.0071 28.5357 27.9528\n",
      "2009-12-18 27.9186 151863506.0 27.5957 27.929 27.5143\n",
      "2009-12-17 27.4086 96720359.0 27.7514 27.8571 27.2857\n"
     ]
    }
   ],
   "source": [
    "# Let's format it a bit\n",
    "for i, row in enumerate(db['aapl']):\n",
    "    if i < 10:\n",
    "        print(f\"{row['date']} {row['close']} {row['volume']} {row['open']} {row['high']} {row['low']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a query to find all the days the stock closed above 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-12-31 30.1046 87907426.0 30.4471 30.4786 30.08\n",
      "2009-12-30 30.2343 102705781.0 29.8328 30.2857 29.7586\n",
      "2009-12-29 29.8714 110755363.0 30.3757 30.3886 29.8186\n",
      "2009-12-28 30.23 160784168.0 30.2457 30.5643 29.9444\n",
      "2009-12-24 29.8628 125222058.0 29.0786 29.9071 29.05\n",
      "2009-12-23 28.8714 86118086.0 28.75 28.9114 28.6871\n",
      "2009-12-22 28.6228 87148416.0 28.4914 28.6928 28.38\n",
      "2009-12-21 28.3186 152166116.0 28.0071 28.5357 27.9528\n",
      "2009-12-18 27.9186 151863506.0 27.5957 27.929 27.5143\n",
      "2009-12-17 27.4086 96720359.0 27.7514 27.8571 27.2857\n"
     ]
    }
   ],
   "source": [
    "high_close = db['aapl'].find(close = {'>=': 27})\n",
    "for i, row in enumerate(high_close):\n",
    "    if i < 10:\n",
    "        print(f\"{row['date']} {row['close']} {row['volume']} {row['open']} {row['high']} {row['low']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's see how to automate that better. OrderedDict has an `items()` function that will give you the key, value pairs out of the dictionary. You can then use a for loop like this:\n",
    "\n",
    "`for key, value in dict.items()`\n",
    "\n",
    "and if you just want a list of the values:\n",
    "\n",
    "`values = [value for key, value in dict.items()]`\n",
    "\n",
    "Let's apply that to our query to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, datetime.date(2009, 12, 31), 30.1046, 87907426.0, 30.4471, 30.4786, 30.08]\n",
      "[2, datetime.date(2009, 12, 30), 30.2343, 102705781.0, 29.8328, 30.2857, 29.7586]\n",
      "[4, datetime.date(2009, 12, 28), 30.23, 160784168.0, 30.2457, 30.5643, 29.9444]\n"
     ]
    }
   ],
   "source": [
    "# Have to run the query each time we use it\n",
    "high_close = db['aapl'].find(close = {'>=': 30})\n",
    "for row in high_close:\n",
    "    vals = [v for k, v in row.items()]\n",
    "    print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we need! We wil use slicing to exclude that index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[datetime.date(2009, 12, 31), 30.1046, 87907426.0, 30.4471, 30.4786, 30.08],\n",
       " [datetime.date(2009, 12, 30),\n",
       "  30.2343,\n",
       "  102705781.0,\n",
       "  29.8328,\n",
       "  30.2857,\n",
       "  29.7586],\n",
       " [datetime.date(2009, 12, 29),\n",
       "  29.8714,\n",
       "  110755363.0,\n",
       "  30.3757,\n",
       "  30.3886,\n",
       "  29.8186],\n",
       " [datetime.date(2009, 12, 28), 30.23, 160784168.0, 30.2457, 30.5643, 29.9444],\n",
       " [datetime.date(2009, 12, 24), 29.8628, 125222058.0, 29.0786, 29.9071, 29.05]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_close = db['aapl'].find(close = {'>=': 27})\n",
    "excel_rows = []\n",
    "\n",
    "# Get the headers\n",
    "# excel_rows.append(db['aapl'].columns[1:])\n",
    "\n",
    "for row in high_close:\n",
    "    vals = [v for k, v in row.items()]\n",
    "    excel_rows.append(vals[1:])\n",
    "\n",
    "excel_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook() # Create a new workbook\n",
    "sheet = workbook.active # Get the active worksheet\n",
    "\n",
    "header = db['aapl'].columns[1:]\n",
    "# The header row comes from the db as a weird type\n",
    "# so we convert to string\n",
    "header = [str(v) for v in header]\n",
    "sheet.append(header)\n",
    "for row in excel_rows:\n",
    "    sheet.append(row)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to write the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"621_demo.xlsx\"\n",
    "\n",
    "workbook.save(filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
